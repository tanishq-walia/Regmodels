---
title: "Regression with PCA and PLS"
author: "Tikam Singh"
date: '2022-01-11'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r , message=FALSE, warning=FALSE, paged.print=FALSE,}

# Helper packages
library(dplyr)     # for data manipulation
library(ggplot2)   # for awesome graphics
library(AmesHousing) # ames data

# Modeling process packages
library(rsample)   # for resampling procedures
library(caret) 
library(pls)      #Partial Least Squares and Principal Component Regression
library(vip)
library(pdp)
```

```{r}
set.seed(123)
ames <- AmesHousing::make_ames()
split <- initial_split(ames, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

```{r}
# perform 10-fold cross validation on a PCR model tuning the 
# number of principal components to use as predictors from 1-100
set.seed(123)
cv_model_pcr <- train(
                      Sale_Price ~ ., 
                      data = ames_train, 
                      method = "pcr",
                      trControl = trainControl(method = "cv", 
                                               number = 10),
                      preProcess = c("zv", "center", "scale"),
                      tuneLength = 100
                      )
cv_model_pcr$bestTune

cv_model_pcr$results %>%
  dplyr::filter(ncomp == pull(cv_model_pcr$bestTune))
ggplot(cv_model_pcr)

```

By controlling for multicollinearity with PCR, we can experience significant improvement in our predictive accuracy compared to the previously obtained linear models (reducing the cross-validated RMSE from about \$37,000 to nearly \$30,000), which beats the k-nearest neighbor model

it simply seeks to reduce the variability present throughout the predictor space. If that variability happens to be related to the response variability, then PCR has a good chance to identify a predictive relationship, as in our case. If, however, the variability in the predictor space is not related to the variability of the response, then PCR can have difficulty identifying a predictive relationship when one might actually exists (i.e., we may actually experience a decrease in our predictive accuracy). An alternative approach to reduce the impact of multicollinearity is partial least squares.

similar to PCR, we can easily fit a PLS model by changing the method argument in train(). As with PCR, the number of principal components to use is a tuning parameter that is determined by the model that maximizes predictive accuracy (minimizes RMSE in this case). The following performs cross-validated PLS with 1 to 30 PCs, and Figure 4.10 shows the cross-validated RMSEs. You can see a greater drop in prediction error compared to PCR and we reach this minimum RMSE with far less principal components because they are guided by the response.

```{r}
# perform 10-fold cross validation on a PLS model tuning the 
# number of principal components to use as predictors from 1-30
set.seed(123)
cv_model_pls <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 30
)

cv_model_pls$bestTune
# results for model with lowest RMSE
cv_model_pls$results %>%
  dplyr::filter(ncomp == pull(cv_model_pls$bestTune))
```

We can use vip::vip() to extract and plot the most important variables. The importance measure is normalized from 100 (most important) to 0 (least important)

```{r}
vip(cv_model_pls, num_features = 20, method = "model")
```

As stated earlier, linear regression models assume a monotonic linear relationship. To illustrate this, we can construct partial dependence plots (PDPs). PDPs plot the change in the average predicted value of Y w.r.t specified set of feature(s) vary over their marginal distribution. PDPs become more useful when non-linear relationships are present. However, PDPs of linear models help illustrate how a fixed change in X(i) relates to a fixed change in y(i),while taking into account the average effect of all the other features in the model

The pdp package provides convenient functions for computing and plotting PDPs.

```{r}
pdp::partial(cv_model_pls, "Gr_Liv_Area", grid.resolution = 20,
                  plot = TRUE)

```

-   Final thoughts Linear regression is usually the first supervised learning algorithm you will learn. The approach provides a solid fundamental understanding of the supervised learning task
