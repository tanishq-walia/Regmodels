---
title: "Notes for CV"
author: "Tikam Singh"
date: '2022-01-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### Packages Required

```{r , message=FALSE, warning=FALSE, paged.print=FALSE,}

# Helper packages
library(dplyr)     # for data manipulation
library(ggplot2)   # for awesome graphics
library(AmesHousing) # ames data

# Modeling process packages
library(rsample)   # for resampling procedures
library(caret)     # for resampling and model training
library(h2o)       # for resampling and model training

# h2o set-up 
h2o.no_progress()  # turn off h2o progress bars
h2o.init()         # launch h2o

pkgs <- c("RCurl","jsonlite","h2o")
     for (pkg in pkgs) 
         {
           if ( !(pkg %in% rownames(installed.packages())))
                  { 
                    install.packages(pkg)
                   }
     }

#test H20

library(h2o)
localH2O = h2o.init()
demo(h2o.kmeans)


```

-   my_libraries \<- c("rsample","modeldata","caret", "h2o", "dplyr", "ggplot2") lapply(my_libraries, require, character.only = T)

```{r pressure, echo=FALSE}
#View(ames_raw)
#View(make_ames)
#Using Base R
ames <- AmesHousing::make_ames()
ames.h2o <- as.h2o(ames)

set.seed(123)  # for reproducibility
index_1 <- sample(1:nrow(ames), round(nrow(ames) * 0.7))
train_1 <- ames[index_1, ]
test_1  <- ames[-index_1, ]



# Using rsample package
split_1  <- initial_split(ames, prop = 0.7)
train_3  <- training(split_1)
test_3   <- testing(split_1)

# Using h2o package
split_2 <- h2o.splitFrame(ames.h2o, ratios = 0.7, 
                          seed = 123)
train_4 <- split_2[[1]]
test_4  <- split_2[[2]]
```

Putting the processes together

To illustrate how this process works together via R code, let's do a simple assessment on the ames housing data. First, we perform stratified sampling as illustrated in Section 2.2.2 to break our data into training vs. test data while ensuring we have consistent distributions between the training and test sets.

##### Stratified sampling with the rsample package

```{r}
set.seed(123)
split <- initial_split(ames, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

Next, we're going to apply a k-nearest neighbor regressor to our data. To do so, we'll use caret, which is a meta-engine to simplify the resampling, grid search, and model application processes. The following defines:

-   Resampling method: we use 10-fold CV repeated 5 times. --\>
-   Grid search: we specify the hyperparameter values to assess (k=2 to 5) --\>
-   Model training & Validation: we train a k-nearest neighbor (method = "knn") model using our pre-specified resampling procedure (trControl = cv), grid search (tuneGrid = hyper_grid), and preferred loss function (metric = "RMSE").

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

# Specify resampling strategy
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
)

# Create grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Tune a knn model using grid search
knn_fit <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
)
# Print and plot the CV results
knn_fit
ggplot(knn_fit)
```

Looking at our results we see that the best model coincided with k= 7, which resulted in an RMSE of 43439.07. This implies that, on average, our model mispredicts the expected sale price of a home by \$43,439. Figure 2.15 illustrates the cross-validated error rate across the spectrum of hyperparameter values that we specified.
