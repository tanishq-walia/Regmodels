---
title: "Feature and Targate Engg"
author: "Tikam Singh"
date: '2022-01-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### Helper packages

```{r}

pkgs<-c("dplyr","ggplot2","visdat","caret","recipes")
 
for(pkg in pkgs) 
   {
     if ( !(pkg %in% rownames(installed.packages())))
           { install.packages(pkg) }
}
library(AmesHousing)
library(dplyr)
library(ggplot2)
library(caret)
library(visdat)
library(recipes)
library(rsample) 
```

-   There are two main approaches to help correct for positively skewed target variables:

1.  Normalize with a log transformation. This will transform most right skewed distributions to be approximately normal. One way to do this is to simply log transform the training and test set in a manual, single step manner similar to : log(ames_train\$Sale_Price)

2.  Use a Box Cox transformation. A Box Cox transformation is more flexible than (but also includes as a special case) the log transformation and will find an appropriate transformation from a family of power transforms that will transform the variable as close as possible to a normal distribution (Box and Cox 1964; Carroll and Ruppert 1981). At the core of the Box Cox transformation is an exponent,

```{r}
set.seed(123)
ames <- AmesHousing::make_ames()
split <- initial_split(ames, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

-   However, we should think of the pre processing as creating a blueprint to be re-applied strategically. For this, you can use the recipe package or something similar (e.g., caret::preProcess()). This will not return the actual log transformed values but, rather, a blueprint to be applied later.

```{r}
hist(ames_train$Sale_Price) # Rightly Skewed
transformed_response <- log(ames_train$Sale_Price)
```

# 

```{r}
ames_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_log(all_outcomes())
ames_recipe
```

At the core of the Box Cox transformation is an exponent, lambda (位位), which varies from -5 to 5. All values of 位位 are considered and the optimal value for the given data is estimated from the training data; The "optimal value" is the one which results in the best transformation to an approximate normal distribution. The transformation of the response YY has the form

##### There are three main steps in creating and applying feature engineering with recipes:

-   recipe: where you define your feature engineering steps to create your blueprint.

-   bake: apply the blueprint to new data

The first step is where you define your blueprint (aka recipe). With this process, you supply the formula of interest (the target variable, features, and the data these are based on) with recipe() and then you sequentially add feature engineering steps with step_xxx(). For example, the following defines Sale_Price as the target variable and then uses all the remaining columns as features based on ames_train. We then:

-    Remove near-zero variance features that are categorical (aka nominal). --\>
-    Ordinal encode our quality-based features (which are inherently ordinal). --\>
-    Center and scale (i.e., standardize) all numeric features. --\>
-    Perform dimension reduction by applying PCA to all numeric features. --\>

```{r}


blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal())  %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_pca(all_numeric(), -all_outcomes())
blueprint
```

Next, we need to train this blueprint on some training data. Remember, there are many feature engineering steps that we do not want to train on the test data (e.g., standardize and PCA) as this would create data leakage. So in this step we estimate these parameters based on the training data of interest.--

```{r}
prepare <- prep(blueprint, training = ames_train)

prepare

```

now,can apply our blueprint to new data (e.g., the training data or future test data) with bake().

```{r}
baked_train <- bake(prepare, new_data = ames_train)
baked_test <- bake(prepare, new_data = ames_test)
baked_train

```

Consequently, the goal is to develop our blueprint, then within each resample iteration we want to apply prep() and bake() to our resample training and validation data. Luckily, the caret package simplifies this process. We only need to specify the blueprint and caret will automatically prepare and bake within each resample. We illustrate with the ames housing example.

1.  First, we create our feature engineering blueprint to perform the following tasks: --
2.  Filter out near-zero variance features for categorical features. --
3.  Ordinally encode all quality features, which are on a 1--10 Likert scale. --
4.  Standardize (center and scale) all numeric features. --
5.  One-hot encode our remaining categorical features. --

```{r}
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

```{r}
# Specify resampling plan
cv <- trainControl(
                    method = "repeatedcv", 
                    number = 10, 
                    repeats = 5
                  )

# Construct grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Tune a knn model using grid search
knn_fit2 <- train(
                    blueprint, 
                    data = ames_train, 
                    method = "knn", 
                    trControl = cv, 
                    tuneGrid = hyper_grid,
                    metric = "RMSE"
                  )
```

```{r}
# print model results
knn_fit2
```

```{r}

# plot cross validation results
ggplot(knn_fit2)
```

Looking at our results we see that the best model was associated with k= 13, which resulted in a cross-validated RMSE of 32,898, illustrates the cross-validated error rate across the spectrum of hyperparameter values that we specified.

-   By applying a handful of the pre processing techniques discussed throughout this chapter, we were able to reduce our prediction error by over \$10,000.

Thinking of feature engineering as a blueprint forces us to think of the ordering of our pre processing steps. Although each particular problem requires you to think of the effects of sequential pre processing, there are some general suggestions that you should consider:

------------------------------------------------------------------------

Sequence:

-   If using a log or Box-Cox transformation, don't center the data first or do any operations that might make the data non-positive. Alternatively, use the Yeo-Johnson transformation so you don't have to worry about this.

-   One-hot or dummy encoding typically results in sparse data which many algorithms can operate efficiently on. If you standardize sparse data you will create dense data and you loose the computational efficiency. Consequently, it's often preferred to standardize your numeric features and then one-hot/dummy encode.

-   If you are lumping infrequently occurring categories together, do so before one-hot/dummy encoding.

-   Although you can perform dimension reduction procedures on categorical features, it is common to primarily do so on numeric features when doing so for feature engineering purposes.

While your project's needs may vary, here is a suggested order of potential steps that should work for most problems:

1.  Filter out zero or near-zero variance features.

2.  Perform imputation if required.

3.  Normalize to resolve numeric feature skewness.

4.  Standardize (center and scale) numeric features.

5.  Perform dimension reduction (e.g., PCA) on numeric features.

6.  One-hot or dummy encode categorical features.
